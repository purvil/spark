{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/purvil/spark-2.4.3-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Aggregation').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spark's core data source\n",
    "    - CSV, JSON, Parquet, ORC, JDBC/ODBC, Plain-text files\n",
    "* Also Cassandra, MongoDB, HBase etc. are supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read API\n",
    "```\n",
    "DataFrameReader.format(...).option(\"key\", \"value\").schema(...).load()\n",
    "```\n",
    "- Default format is Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can get DataFrameeader using `spark.read`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "spark.read.format(\"csv\")\n",
    "    .option(\"mode\", \"FAILFAST\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"path\", \"path/to/file(s)\")\n",
    "    .schema(someSchema)\n",
    "    .load()\n",
    "```\n",
    "* `mode` : What will happen when spark find malformed data source.\n",
    "    - `permissive`: default, set all malformed field to null\n",
    "    - `dropMalformed` : Drop rows with malformed recors\n",
    "    - `failFast` : Fails immediately\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write data\n",
    "\n",
    "```\n",
    "DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By default format for writing is arquet.\n",
    "* DataFrameWriter is shown as \n",
    "```\n",
    "dataframe.write.format(\"crv\").option(\"mode\", \"OVERWRITE\").option(\"dateFormat\", \"yyyy-MM-dd\").option(\"path\", \"path to file\").save()\n",
    "```\n",
    "* Mode are \n",
    "    * `append` : Appends the output files to the list of files that already exist at that location\n",
    "    * `overwrite` : Completely overwrite any data that already exist there\n",
    "    * `errorIfExists` : Throws error and fail the write if file exists (default)\n",
    "    * `ignore` : If data or file exist at the location, do nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/csv1.png)\n",
    "![](images/csv2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Even if data does not fit with specified schema or file does not exist, it will fail at job execution time, not at DataFrame definition time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"mode\", \"FAILFAST\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"spark_data/flight-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\"my-tsv-file.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-8c9c7127-f960-440f-9363-fcef7ea7af1f-c000.csv  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls my-tsv-file.tsv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of partition of dataframe at the time of writing = number of files in folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In Spark we generally use line delimited JSON files\n",
    "* Other type is large JSON object or array per file.\n",
    "* When `multiline` option is true we read an entire file as one JSON file.\n",
    "* Line delimited JSON, allows to append new records.\n",
    "![](images/json1.png)\n",
    "![](images/json2.png)\n",
    "![](images/json3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"json\").option(\"mode\", \"FAILFAST\").option(\"inferSchema\", \"true\").load(\"spark_data/flight-data/json/2010-summary.json\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Writing JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"json\").mode(\"overwrite\").save(\"my-json.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-fda4046a-2921-466b-ab2c-4d684c93c746-c000.json  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls my-json.json/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquest\n",
    "* Open source column oriented data store, provides storage optimization\n",
    "* Provides columnar compression, which saves storage and alloews for reading individual column\n",
    "* Spark's default file format.\n",
    "![](images/parquet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"parquet\").load(\"spark_data/flight-data/parquet/2010-summary.parquet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"parquet\").mode(\"overwrite\").save(\"my-parquet-file.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORC\n",
    "* Self describing, type aware columnar file for Hadoop. Optimized for large streaming read, with support for finding required rows quickly.\n",
    "* Parquet is optimized for Spark and ORC is for Hadoop and Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"orc\").load(\"spark_data/flight-data/orc/2010-summary.orc/\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"orc\").mode(\"overwrite\").save(\"my-orc.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each line in file becomes record in DattaFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can control parallelism of files that we write by controlling the partitions prior to writing.\n",
    "* Splittable files\n",
    "    - Certain file formats are fundamentally splittable. Which avoid reading an entire file.\n",
    "    - In HDFS splitted file yield more optimization if it is saved in multiple block\n",
    "* Not all compression schema are splittable. Using Parquet with gzip compression is the best.\n",
    "* Multiple executors can not read from same file at the same time, but they can read different file at the same time. When we read folder with multiple file in it, each one of those files will become a partition in dataframe and be read in by available in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of files written is dependent on the number of partitions the dataframe has at the time we write the data.\n",
    "* One file is written per partition of data by default.\n",
    "* So, if we specify a filename it will be number of files in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.repartition(5).write.format(\"csv\").save(\"multiple.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-075f0b09-4e88-41dd-ba11-73ef7898213e-c000.csv\r\n",
      "part-00001-075f0b09-4e88-41dd-ba11-73ef7898213e-c000.csv\r\n",
      "part-00002-075f0b09-4e88-41dd-ba11-73ef7898213e-c000.csv\r\n",
      "part-00003-075f0b09-4e88-41dd-ba11-73ef7898213e-c000.csv\r\n",
      "part-00004-075f0b09-4e88-41dd-ba11-73ef7898213e-c000.csv\r\n",
      "_SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "! ls multiple.csv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Partitioning is a tool that allows us to control what and where data is stored.\n",
    "* When we write data to a paritioned directory, we encode column as a folder. This allows to skip lots of data when we go to read it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.limit(10).write.format(\"parquet\").mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\").save(\"partitioned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'DEST_COUNTRY_NAME=Costa Rica'\t'DEST_COUNTRY_NAME=Senegal'\r\n",
      "'DEST_COUNTRY_NAME=Egypt'\t'DEST_COUNTRY_NAME=United States'\r\n",
      "'DEST_COUNTRY_NAME=Moldova'\t _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls partitioned.parquet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each file has data where previous predicate was true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-c726aa7a-72a2-4d5f-901c-4184a3f70f10.c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls partitioned.parquet/DEST_COUNTRY_NAME\\=Senegal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Frequent filters like a date are good candidate for partition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing\n",
    "* We can control the data that is specifically written to each file.\n",
    "* Which can avoid shuffles, because data with same bucket id will all be grouped together into one physical partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When we parition one specific column, we might write out so many directories. Bucketing will create a certain number of files and organize our data into those buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberBuckets = 10\n",
    "columnToBucketBy = \"count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"parquet\").mode(\"overwrite\").bucketBy(numberBuckets, columnToBucketBy).saveAsTable(\"bucketedFile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'bucketedFile': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! ls bucketedFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Writing lots of small file can create lots of metadata. Having lage file will lead us to read entire block of data when we want only few rows.\n",
    "* `maxRecordsPerFile` allows us to control file size. `df.write.option(\"maxRecordsPerFile\", 5000)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
